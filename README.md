# AI-Powered-Text-Summarizer-App
A lightweight AI app that allows users to input text (500 words or more) and provides a three-bullet-point summary leveraging a large language model

# ğŸ¤– AI Text Summarizer

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red.svg)](https://streamlit.io/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Hugging Face](https://img.shields.io/badge/ğŸ¤—-Hugging%20Face-yellow)](https://huggingface.co/)

A lightweight AI-powered web application that generates concise **3-bullet-point summaries** from long-form text using Meta's **Llama 3.2 1B Instruct** model. Perfect for quickly distilling key insights from articles, reports, documents, and other lengthy content.

## âœ¨ Features

- ğŸ“ **Smart Summarization**: Generate exactly 3 comprehensive bullet points from any text
- âš¡ **Fast Processing**: Optimized inference with typical response times under 5 seconds
- ğŸ¯ **Quality Control**: Enforces minimum 500-word input for meaningful summarization
- ğŸ’» **Local Processing**: Runs entirely on your machine - no API keys or external calls
- ğŸ“± **Responsive UI**: Clean, intuitive Streamlit interface with real-time feedback
- ğŸ“Š **Performance Tracking**: Built-in processing time monitoring and usage statistics
- ğŸ’¾ **Export Capability**: Download summaries as text files
- ğŸ”§ **Hardware Adaptive**: Automatically detects and utilizes GPU when available

## ğŸš€ Quick Start

### Prerequisites

- Python 3.9 or higher
- 4GB+ RAM (8GB recommended)
- Optional: CUDA-compatible GPU for faster processing

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/ai-text-summarizer.git
   cd ai-text-summarizer
   ```

2. **Create virtual environment** (recommended)
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Launch the application**
   ```bash
   streamlit run app.py
   ```

5. **Open your browser** to `http://localhost:8501`

That's it! The Llama model will download automatically on first use (~2.5GB).

## ğŸ“– Usage

1. **Input Text**: Paste or type your long-form text (minimum 500 words)
2. **Validate**: The word counter shows your progress toward the 500-word minimum
3. **Generate**: Click "ğŸš€ Generate Summary" to create your bullet points
4. **Review**: View your 3-bullet summary with processing time
5. **Export**: Download the summary as a text file if needed

### Example Input/Output

**Input** (excerpt from a technology article):
> "Artificial intelligence has revolutionized the way we approach problem-solving across industries... [500+ words about AI impact, applications, challenges, and future prospects]"

**Output**:
â€¢ Artificial intelligence has transformed multiple industries by automating complex decision-making processes and enabling data-driven insights that were previously impossible to achieve at scale.

â€¢ Key applications span from healthcare diagnostics and financial fraud detection to autonomous vehicles and personalized recommendation systems, demonstrating AI's versatility across sectors.

â€¢ Despite significant advances, challenges remain in AI ethics, bias mitigation, and regulatory frameworks, requiring careful consideration as the technology continues to evolve rapidly.

## ğŸ—ï¸ Architecture

### System Overview
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User      â”‚â”€â”€â”€â–¶â”‚  Streamlit   â”‚â”€â”€â”€â–¶â”‚ TextSummarizer  â”‚
â”‚   Input     â”‚    â”‚   Frontend   â”‚    â”‚   (Backend)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚                       â”‚
                          â”‚                       â–¼
                          â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚            â”‚ Llama 3.2 1B    â”‚
                          â”‚            â”‚ Instruct Model  â”‚
                          â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚                       â”‚
                          â–¼                       â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   Results   â”‚â—€â”€â”€â”€â”‚ Post-processing â”‚
                   â”‚   Display   â”‚    â”‚ & Validation    â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Components

- **`app.py`**: Streamlit frontend with user interface and session management
- **`summarizer.py`**: Core TextSummarizer class handling AI model operations
- **`config.py`**: Centralized configuration for model parameters and app settings
- **`requirements.txt`**: Python dependencies specification

## âš™ï¸ Configuration

### Model Parameters
```python
MODEL_CONFIG = {
    "model_name": "meta-llama/Llama-3.2-1B-Instruct",
    "max_length": 300,      # Maximum tokens for summary
    "temperature": 0.7,     # Creativity vs consistency balance
    "top_p": 0.9,          # Nucleus sampling parameter
    "top_k": 50,           # Vocabulary restriction
}
```

### Application Settings
```python
APP_CONFIG = {
    "min_words": 500,      # Minimum input word count
    "max_words": 5000,     # Recommended maximum
    "cache_model": True,   # Enable model caching
}
```

## ğŸ› ï¸ Development

### Project Structure
```
ai-text-summarizer/
â”œâ”€â”€ app.py                 # Streamlit frontend application
â”œâ”€â”€ summarizer.py          # Core AI summarization logic
â”œâ”€â”€ config.py              # Configuration settings
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ README.md             # This file
â”œâ”€â”€ tests/                # Unit and integration tests
â”‚   â”œâ”€â”€ test_summarizer.py
â”‚   â””â”€â”€ test_app.py
â””â”€â”€ docs/                 # Additional documentation
    â””â”€â”€ architecture.md
```

### Running Tests
```bash
# Install test dependencies
pip install pytest pytest-cov

# Run all tests
pytest

# Run with coverage report
pytest --cov=. --cov-report=html
```

### Code Quality
```bash
# Format code
black .

# Check style
flake8 .

# Sort imports
isort .
```

## ğŸš€ Deployment

### Local Development
```bash
streamlit run app.py
```

### Docker Deployment
```dockerfile
FROM python:3.9-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 8501

CMD ["streamlit", "run", "app.py", "--server.address", "0.0.0.0"]
```

### Cloud Deployment Options

| Platform | Pros | Cons |
|----------|------|------|
| **Streamlit Cloud** | Free, Easy setup, GitHub integration | Limited resources |
| **Heroku** | Simple deployment, Good for demos | Dyno limitations |
| **AWS EC2** | Full control, GPU support | Requires setup |
| **Google Cloud Run** | Serverless, Auto-scaling | Cold starts |

## ğŸ“Š Performance

### Benchmarks
| Metric | Specification |
|--------|---------------|
| **Model Size** | ~2.5GB download |
| **Memory Usage** | 2-4GB RAM |
| **Inference Time** | 3-7 seconds (CPU), 1-3 seconds (GPU) |
| **Model Load Time** | 15-25 seconds (cached after first load) |
| **Supported Input** | Up to 8,192 tokens (~6,000 words) |

### Hardware Requirements
| Component | Minimum | Recommended |
|-----------|---------|-------------|
| **RAM** | 4GB | 8GB+ |
| **CPU** | 2 cores | 4+ cores |
| **GPU** | Not required | GTX 1060+ |
| **Storage** | 5GB free | 10GB+ |

## ğŸ¤” Why Llama 3.2 1B?

### âœ… Advantages
- **Optimal Balance**: Great performance-to-size ratio for summarization tasks
- **Local Processing**: No API costs or rate limits
- **Fast Inference**: Quick response times suitable for interactive use
- **Instruction Following**: Excellent at structured output generation
- **Resource Efficient**: Runs on modest hardware configurations

### ğŸ“ˆ Model Comparison
| Model | Size | Quality | Speed | Resource Usage |
|-------|------|---------|-------|----------------|
| **Llama 3.2 1B** | â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |
| GPT-3.5 Turbo | N/A | â­â­â­â­â­ | â­â­â­ | â­â­â­â­â­ |
| Llama 3.2 3B | â­â­ | â­â­â­â­â­ | â­â­â­ | â­â­â­ |
| T5-Small | â­â­â­â­â­ | â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |

## ğŸ§ª Testing

### Test Coverage
- âœ… Unit tests for core summarization logic
- âœ… Integration tests for end-to-end pipeline
- âœ… UI component testing
- âœ… Performance benchmarking
- âœ… Error handling validation

### Sample Test
```python
def test_summarization_output():
    """Test that summarizer produces exactly 3 bullet points"""
    summarizer = TextSummarizer()
    sample_text = "Long text content..." * 100  # 500+ words
    
    result = summarizer.summarize_to_bullets(sample_text)
    
    assert len(result) == 3
    assert all(len(bullet.strip()) > 10 for bullet in result)
    assert not any("<|eot_id|>" in bullet for bullet in result)
```

## ğŸ”§ Troubleshooting

### Common Issues

**Model Loading Errors**
```bash
# Clear Hugging Face cache
rm -rf ~/.cache/huggingface/

# Reinstall transformers
pip uninstall transformers
pip install transformers>=4.35.0
```

**Memory Issues**
- Reduce `max_length` in `config.py`
- Use CPU-only mode: set `CUDA_VISIBLE_DEVICES=""`
- Close other applications to free RAM

**Slow Performance**
- Ensure GPU drivers are installed for CUDA support
- Check available memory with `nvidia-smi`
- Consider using smaller input texts

### Getting Help
- ğŸ“– Check the [documentation](docs/)
- ğŸ› Open an [issue](https://github.com/yourusername/ai-text-summarizer/issues)
- ğŸ’¬ Start a [discussion](https://github.com/yourusername/ai-text-summarizer/discussions)

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

### Development Setup
1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Make your changes and add tests
4. Run the test suite: `pytest`
5. Commit your changes: `git commit -m 'Add amazing feature'`
6. Push to the branch: `git push origin feature/amazing-feature`
7. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Meta AI** for the Llama 3.2 1B Instruct model
- **Hugging Face** for the transformers library and model hosting
- **Streamlit** for the excellent web app framework
- **Python Community** for the amazing ecosystem of ML libraries

## ğŸ“š Citation

If you use this project in your research or work, please cite:

```bibtex
@software{ai_text_summarizer,
  title={AI Text Summarizer: Llama 3.2 1B-powered Bullet Point Generation},
  author={Your Name},
  year={2025},
  url={https://github.com/yourusername/ai-text-summarizer}
}
```

---

<div align="center">

**Built with â¤ï¸ using Python, Streamlit, and Llama 3.2 1B**

[â­ Star this repo](https://github.com/yourusername/ai-text-summarizer) â€¢ [ğŸ› Report Bug](https://github.com/yourusername/ai-text-summarizer/issues) â€¢ [ğŸ’¡ Request Feature](https://github.com/yourusername/ai-text-summarizer/issues)

</div>
